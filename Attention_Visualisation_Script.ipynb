{"cells":[{"cell_type":"markdown","source":["### Adapted from: https://colab.research.google.com/github/neelnanda-io/Easy-Transformer/blob/main/Exploratory_Analysis_Demo.ipynb#scrollTo=2EmZ15ejPTVo"],"metadata":{"id":"ROucKjfb9Ced"}},{"cell_type":"markdown","metadata":{"id":"pGI_F1bIvOhl"},"source":["# Setup\n","(No need to read)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cEfVibEmvOhm"},"outputs":[],"source":["# Janky code to do different setup when run in a Colab notebook vs VSCode\n","DEBUG_MODE = False\n","try:\n","    import google.colab\n","    IN_COLAB = True\n","    print(\"Running as a Colab notebook\")\n","    %pip install git+https://github.com/neelnanda-io/TransformerLens.git\n","    # Install another version of node that makes PySvelte work way faster\n","    !curl -fsSL https://deb.nodesource.com/setup_16.x | sudo -E bash -; sudo apt-get install -y nodejs\n","    %pip install git+https://github.com/neelnanda-io/PySvelte.git\n","except:\n","    IN_COLAB = False\n","    print(\"Running as a Jupyter notebook - intended for development only!\")\n","    from IPython import get_ipython\n","\n","    ipython = get_ipython()\n","    # Code to automatically update the HookedTransformer code as its edited without restarting the kernel\n","    ipython.magic(\"load_ext autoreload\")\n","    ipython.magic(\"autoreload 2\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1djO3tiDKJzn"},"outputs":[],"source":["!curl -fsSL https://deb.nodesource.com/setup_16.x | sudo -E bash -; sudo apt-get install -y nodejs\n","!pip install git+https://github.com/neelnanda-io/PySvelte.git"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-SxIL99gvOhn"},"outputs":[],"source":["# Plotly needs a different renderer for VSCode/Notebooks vs Colab argh\n","import plotly.io as pio\n","\n","if IN_COLAB or not DEBUG_MODE:\n","    # Thanks to annoying rendering issues, Plotly graphics will either show up in colab OR Vscode depending on the renderer - this is bad for developing demos! Thus creating a debug mode.\n","    pio.renderers.default = \"colab\"\n","else:\n","    pio.renderers.default = \"png\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bGlcI_prvOhn"},"outputs":[],"source":["# Import stuff\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import numpy as np\n","import einops\n","from fancy_einsum import einsum\n","import tqdm.notebook as tqdm\n","import random\n","from pathlib import Path\n","import plotly.express as px\n","from torch.utils.data import DataLoader\n","\n","from torchtyping import TensorType as TT\n","from typing import List, Union, Optional\n","from functools import partial\n","import copy\n","\n","import itertools\n","from transformers import AutoModelForCausalLM, AutoConfig, AutoTokenizer\n","import dataclasses\n","import datasets\n","from IPython.display import HTML"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"09-Rhhi_vOho"},"outputs":[],"source":["import pysvelte\n","\n","import transformer_lens\n","import transformer_lens.utils as utils\n","from transformer_lens.hook_points import (\n","    HookedRootModule,\n","    HookPoint,\n",")  # Hooking utilities\n","from transformer_lens import HookedTransformer, HookedTransformerConfig, FactoredMatrix, ActivationCache"]},{"cell_type":"markdown","metadata":{"id":"DMN-l1QBvOho"},"source":["We turn automatic differentiation off, to save GPU memory, as this notebook focuses on model inference not model training."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6XMfqnbTvOho"},"outputs":[],"source":["torch.set_grad_enabled(False)"]},{"cell_type":"markdown","metadata":{"id":"orN8ArG-vOho"},"source":["Plotting helper functions:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1kfEFq_wvOhp"},"outputs":[],"source":["def imshow(tensor, renderer=None, **kwargs):\n","    px.imshow(utils.to_numpy(tensor), color_continuous_midpoint=0.0, color_continuous_scale=\"RdBu\", **kwargs).show(renderer)\n","\n","def line(tensor, renderer=None, **kwargs):\n","    px.line(y=utils.to_numpy(tensor), **kwargs).show(renderer)\n","\n","def scatter(x, y, xaxis=\"\", yaxis=\"\", caxis=\"\", renderer=None, **kwargs):\n","    x = utils.to_numpy(x)\n","    y = utils.to_numpy(y)\n","    px.scatter(y=y, x=x, labels={\"x\":xaxis, \"y\":yaxis, \"color\":caxis}, **kwargs).show(renderer)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uxRzgFHLvOhp"},"outputs":[],"source":["line(np.arange(5))"]},{"cell_type":"markdown","metadata":{"id":"Kb77x3rovOhp"},"source":["# Pronoun prediction\n","\n"]},{"cell_type":"markdown","metadata":{"id":"0KTlE08ivOhp"},"source":["The first step is to load in our model, GPT-2 Small, a 12 layer and 80M parameter transformer with `HookedTransformer.from_pretrained`. The various flags are simplifications that preserve the model's output but simplify its internals"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yPhNnjEpvOhq"},"outputs":[],"source":["model = HookedTransformer.from_pretrained(\n","    \"gpt2-small\",\n","    center_unembed=True,\n","    center_writing_weights=True,\n","    fold_ln=True,\n","    refactor_factored_attn_matrices=True,\n",")"]},{"cell_type":"markdown","metadata":{"id":"IOgeLVIOvOhq"},"source":["The next step is to verify that the model can *actually* do the task! Here we use `utils.test_prompt`, and see that the model is significantly better at predicting he than she \n","\n","<details><summary>Asides:</summary>\n","\n","Note: If we were being careful, we'd want to run the model on a range of prompts and find the average performance\n","\n","`prepend_bos` is a flag to add a BOS (beginning of sequence) to the start of the prompt. GPT-2 was not trained with this, but I find that it often makes model behaviour more stable, as the first token is treated weirdly.\n","</details>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"j1Sz-ETwvOhq"},"outputs":[],"source":["example_prompt = \"John is such a great friend, isn't\"\n","example_answer = \" he\"\n","utils.test_prompt(example_prompt, example_answer, model, prepend_bos=True)"]},{"cell_type":"markdown","metadata":{"id":"iYpLY-pPvOhq"},"source":["We now want to find a reference prompt to run the model on. \n","\n","We'll run the model on 4 instances of this task, each prompt given twice - one with \"he\" as the first pronoun, one with \"she\" as a second pronoun. To make our lives easier, we'll carefully choose prompts with single token names and the corresponding names in the same token positions.\n","\n","<details> <summary>(*) <b>Aside on tokenization</b></summary>\n","\n","We want models that can take in arbitrary text, but models need to have a fixed vocabulary. So the solution is to define a vocabulary of **tokens** and to deterministically break up arbitrary text into tokens. Tokens are, essentially, subwords, and are determined by finding the most frequent substrings - this means that tokens vary a lot in length and frequency! \n","\n","Tokens are a *massive* headache and are one of the most annoying things about reverse engineering language models... Different names will be different numbers of tokens, different prompts will have the relevant tokens at different positions, different prompts will have different total numbers of tokens, etc. Language models often devote significant amounts of parameters in early layers to convert inputs from tokens to a more sensible internal format (and do the reverse in later layers). You really, really want to avoid needing to think about tokenization wherever possible when doing exploratory analysis (though, of course, it's relevant later when trying to flesh out your analysis and make it rigorous!). HookedTransformer comes with several helper methods to deal with tokens: `to_tokens, to_string, to_str_tokens, to_single_token, get_token_position`\n","\n","**Exercise:** I recommend using `model.to_str_tokens` to explore how the model tokenizes different strings. In particular, try adding or removing spaces at the start, or changing capitalization - these change tokenization!</details>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eSNej0ayKJzr"},"outputs":[],"source":["templates = [\n","    \"So {name} is a really great friend, isn't\",\n","    \"So {name} is such a good cook, isn't\",\n","    \"So {name} is a very good athlete, isn't\",\n","    \"So {name} is a really nice person, isn't\",\n","    \"So {name} is such a funny person, isn't\"\n","    ]\n","\n","male_names = [\n","    \"John\",\n","    \"David\",\n","    \"Mark\",\n","    \"Paul\",\n","    \"Ryan\",\n","    \"Gary\",\n","    \"Jack\",\n","    \"Sean\",\n","    \"Carl\",\n","    \"Joe\",    \n","]\n","female_names = [\n","    \"Mary\",\n","    \"Lisa\",\n","    \"Anna\",\n","    \"Sarah\",\n","    \"Amy\",\n","    \"Carol\",\n","    \"Karen\",\n","    \"Susan\",\n","    \"Julie\",\n","    \"Judy\"\n","]\n","\n","prompts = []\n","correct = []\n","incorrect = []\n","\n","responses = [' he', ' she']\n","\n","count = 0\n","\n","for name in male_names + female_names:\n","    for template in templates:\n","        cur_sentence = template.format(name = name)\n","        prompts.append(cur_sentence)\n","\n","batch_size = len(prompts)\n","\n","count = 0\n","\n","answers = []\n","answer_tokens = []\n","\n","for _ in range(batch_size):\n","    if count < (0.5 * len(prompts)):\n","        answers.append((responses[0], responses[1]))\n","        answer_tokens.append([model.to_single_token(responses[0]), model.to_single_token(responses[1])])\n","        count += 1\n","    else:\n","        answers.append((responses[1], responses[0]))\n","        answer_tokens.append([model.to_single_token(responses[1]), model.to_single_token(responses[0])])\n","\n","answer_tokens = torch.tensor(answer_tokens).cuda()\n","\n","print(prompts)\n","print(answers)\n","print(answer_tokens)"]},{"cell_type":"markdown","metadata":{"id":"ZTUmqFjRvOhq"},"source":["We now run the model on these prompts and use `run_with_cache` to get both the logits and a cache of all internal activations for later analysis"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mYvlWufFvOhr"},"outputs":[],"source":["tokens = model.to_tokens(prompts, prepend_bos=True)\n","# Move the tokens to the GPU\n","tokens = tokens.cuda()\n","# Run the model and cache all activations\n","original_logits, cache = model.run_with_cache(tokens)"]},{"cell_type":"markdown","metadata":{"id":"iQ0iarkkvOhr"},"source":["We'll later be evaluating how model performance differs upon performing various interventions, so it's useful to have a metric to measure model performance. Our metric here will be the **logit difference**, the difference in logit between the indirect object's name and the subject's name (eg, `logit(he)-logit(she)`). "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yT9I8_Z6vOhr"},"outputs":[],"source":["def logits_to_ave_logit_diff(logits, answer_tokens, per_prompt=False):\n","    # Only the final logits are relevant for the answer\n","    final_logits = logits[:, -1, :]\n","    answer_logits = final_logits.gather(dim=-1, index=answer_tokens)\n","    answer_logit_diff = answer_logits[:, 0] - answer_logits[:, 1]\n","    if per_prompt:\n","        return answer_logit_diff\n","    else:\n","        return answer_logit_diff.mean()\n","\n","print(\"Per prompt logit difference:\", logits_to_ave_logit_diff(original_logits, answer_tokens, per_prompt=True))\n","original_average_logit_diff = logits_to_ave_logit_diff(original_logits, answer_tokens)\n","print(\"Average logit difference:\", logits_to_ave_logit_diff(original_logits, answer_tokens).item())"]},{"cell_type":"markdown","metadata":{"id":"rE0-BLN9vOhr"},"source":["# Direct Logit Attribution"]},{"cell_type":"markdown","metadata":{"id":"GQ1RXDO9vOhr"},"source":["Getting an output logit is equivalent to projecting onto a direction in the residual stream. We use `model.tokens_to_residual_directions` to map the answer tokens to that direction, and then convert this to a logit difference direction for each batch"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"K1g7CIagvOhr"},"outputs":[],"source":["answer_residual_directions = model.tokens_to_residual_directions(answer_tokens)\n","print(\"Answer residual directions shape:\", answer_residual_directions.shape)\n","logit_diff_directions = answer_residual_directions[:, 0] - answer_residual_directions[:, 1]\n","print(\"Logit difference directions shape:\", logit_diff_directions.shape)"]},{"cell_type":"markdown","metadata":{"id":"XR7vdLiuvOhs"},"source":["To verify that this works, we can apply this to the final residual stream for our cached prompts (after applying LayerNorm scaling) and verify that we get the same answer. \n","\n","<details> <summary>Technical details</summary>\n","\n","`logits = Unembed(LayerNorm(final_residual_stream))`, so we technically need to account for the centering, and then learned translation and scaling of the layernorm, not just the variance 1 scaling. \n","\n","The centering is accounted for with the preprocessing flag `center_writing_weights` which ensures that every weight matrix writing to the residual stream has mean zero. \n","\n","The learned scaling is folded into the unembedding weights `model.unembed.W_U` via `W_U_fold = layer_norm.weights[:, None] * unembed.W_U`\n","\n","The learned translation is folded to `model.unembed.b_U`, a bias added to the logits (note that GPT-2 is not trained with an existing `b_U`). This roughly represents unigram statistics. But we can ignore this because each prompt occurs twice with names in the opposite order, so this perfectly cancels out. \n","\n","Note that rather than using layernorm scaling we could just study cache[\"ln_final.hook_normalised\"]\n","\n","</details>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UByCi_SUvOhs"},"outputs":[],"source":["# cache syntax - resid_post is the residual stream at the end of the layer, -1 gets the final layer. The general syntax is [activation_name, layer_index, sub_layer_type]. \n","final_residual_stream = cache[\"resid_post\", -1]\n","print(\"Final residual stream shape:\", final_residual_stream.shape)\n","final_token_residual_stream = final_residual_stream[:, -1, :]\n","# Apply LayerNorm scaling\n","# pos_slice is the subset of the positions we take - here the final token of each prompt\n","scaled_final_token_residual_stream = cache.apply_ln_to_stack(final_token_residual_stream, layer = -1, pos_slice=-1)\n","\n","average_logit_diff = einsum(\"batch d_model, batch d_model -> \", scaled_final_token_residual_stream, logit_diff_directions)/len(prompts)\n","print(\"Calculated average logit diff:\", average_logit_diff.item())\n","print(\"Original logit difference:\",original_average_logit_diff.item())"]},{"cell_type":"markdown","metadata":{"id":"QHCJSYwBvOhs"},"source":["## Logit Lens"]},{"cell_type":"markdown","metadata":{"id":"6AFbYbzZvOhs"},"source":["We can now decompose the residual stream! First we apply a technique called the [**logit lens**](https://www.alignmentforum.org/posts/AcKRB8wDpdaN6v6ru/interpreting-gpt-the-logit-lens) - this looks at the residual stream after each layer and calculates the logit difference from that. This simulates what happens if we delete all subsequence layers. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hjAXMiNKvOhs"},"outputs":[],"source":["def residual_stack_to_logit_diff(residual_stack: TT[\"components\", \"batch\", \"d_model\"], cache: ActivationCache) -> float:\n","    scaled_residual_stack = cache.apply_ln_to_stack(residual_stack, layer = -1, pos_slice=-1)\n","    return einsum(\"... batch d_model, batch d_model -> ...\", scaled_residual_stack, logit_diff_directions)/len(prompts)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4lzc5pudvOhs"},"outputs":[],"source":["accumulated_residual, labels = cache.accumulated_resid(layer=-1, incl_mid=True, pos_slice=-1, return_labels=True)\n","logit_lens_logit_diffs = residual_stack_to_logit_diff(accumulated_residual, cache)\n","line(logit_lens_logit_diffs, x=np.arange(model.cfg.n_layers*2+1)/2, hover_name=labels, title=\"Logit Difference From Accumulate Residual Stream\")"]},{"cell_type":"markdown","metadata":{"id":"_kjqdXoZvOhs"},"source":["## Layer Attribution"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ntc8uSqavOht"},"outputs":[],"source":["per_layer_residual, labels = cache.decompose_resid(layer=-1, pos_slice=-1, return_labels=True)\n","per_layer_logit_diffs = residual_stack_to_logit_diff(per_layer_residual, cache)\n","line(per_layer_logit_diffs, hover_name=labels, title=\"Logit Difference From Each Layer\")"]},{"cell_type":"markdown","metadata":{"id":"4HNOo5YhvOht"},"source":["## Head Attribution"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b-8h9IpLvOht"},"outputs":[],"source":["per_head_residual, labels = cache.stack_head_results(layer=-1, pos_slice=-1, return_labels=True)\n","per_head_logit_diffs = residual_stack_to_logit_diff(per_head_residual, cache)\n","per_head_logit_diffs = einops.rearrange(per_head_logit_diffs, \"(layer head_index) -> layer head_index\", layer=model.cfg.n_layers, head_index=model.cfg.n_heads)\n","imshow(per_head_logit_diffs, labels={\"x\":\"Head\", \"y\":\"Layer\"}, title=\"Logit Difference From Each Head\")"]},{"cell_type":"markdown","metadata":{"id":"B8ZXkOBOvOht"},"source":["## Attention Analysis"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mou5qTAFvOht"},"outputs":[],"source":["def visualize_attention_patterns(\n","    heads: Union[List[int], int, TT[\"heads\"]], \n","    local_cache: Optional[ActivationCache]=None, \n","    local_tokens: Optional[torch.Tensor]=None, \n","    title: str=\"\"):\n","    # Heads are given as a list of integers or a single integer in [0, n_layers * n_heads)\n","    if isinstance(heads, int):\n","        heads = [heads]\n","    elif isinstance(heads, list) or isinstance(heads, torch.Tensor):\n","        heads = utils.to_numpy(heads)\n","    # Cache defaults to the original activation cache\n","    if local_cache is None:\n","        local_cache = cache\n","    # Tokens defaults to the tokenization of the first prompt (including the BOS token)\n","    if local_tokens is None:\n","        # The tokens of the first prompt\n","        local_tokens = tokens[0]\n","    \n","    labels = []\n","    patterns = []\n","    batch_index = 0\n","    for head in heads:\n","        layer = head // model.cfg.n_heads\n","        head_index = head % model.cfg.n_heads\n","        # Get the attention patterns for the head\n","        # Attention patterns have shape [batch, head_index, query_pos, key_pos]\n","        patterns.append(local_cache[\"attn\", layer][batch_index, head_index])\n","        labels.append(f\"L{layer}H{head_index}\")\n","    str_tokens = model.to_str_tokens(local_tokens)\n","    patterns = torch.stack(patterns, dim=-1)\n","    # Plot the attention patterns\n","    attention_vis = pysvelte.AttentionMulti(attention=patterns, tokens=str_tokens, head_labels=labels)\n","    display(HTML(f\"<h3>{title}</h3>\"))\n","    attention_vis.show()"]},{"cell_type":"markdown","metadata":{"id":"K6651n3vvOht"},"source":["\n","**Instructions for using the attention visualization**:\n","* Hover over a token to see what tokens it attends to (ie which previous tokens it copies information from) and click on a token to lock focus onto it\n","* By default, colors are the max attention over all tokens, mixed together for each head and its colour.\n","* Each head has its own colour. Hover over a head to just show that head's colour and click to lock.\n","* The grid in the top left shows the attention patterns as a heat map (it's a lower triangular grid because GPT-2 has causal attention - tokens can only attend backwards!). Y axis is the destination token, X axis is the source token.\n","* Click the tick box to flip the direction - now hovering over a token shows which tokens *attend to it* (ie which tokens it copies information to)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IEHGl2WkvOht"},"outputs":[],"source":["top_k = 3\n","top_positive_logit_attr_heads = torch.topk(per_head_logit_diffs.flatten(), k=top_k).indices\n","visualize_attention_patterns(top_positive_logit_attr_heads, title=f\"Top {top_k} Positive Logit Attribution Heads\")\n","top_negative_logit_attr_heads = torch.topk(-per_head_logit_diffs.flatten(), k=top_k).indices\n","visualize_attention_patterns(top_negative_logit_attr_heads, title=f\"Top {top_k} Negative Logit Attribution Heads\")"]},{"cell_type":"code","source":["print(top_positive_logit_attr_heads)\n","print(model.cfg.n_heads)"],"metadata":{"id":"jVYGfxM_e26l"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bonvBwMLvOhx"},"source":["# Visualizing Attention Patterns\n","\n","Let's take some relevant heads from the automatic circuit discovery and split it into early, middle and late heads"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ndm1Eez0vOhx"},"outputs":[],"source":["# early_heads: 0.4, 1.8, 1.10\n","# other early: 1.4, 2.6, 3.6\n","# mid heads: 4.3, 6.0\n","# late heads: 9.7, 10.9, 11.8\n","# head = layer * n_heads + head_index\n","n_heads = 12\n","early_heads = [4, 1*12+8, 1*12+4, 2*12 + 6, 3*12 +6, 1*12 +10]\n","mid_heads = [4*12 +3, 6*12 +0]\n","late_heads = [9*12 + 7, 10*12+9, 11*12+8]\n","visualize_attention_patterns(early_heads, title=f\"Top Early Heads\")\n","visualize_attention_patterns(mid_heads, title=f\"Top Middle Heads\")\n","visualize_attention_patterns(late_heads, title=f\"Top Late Heads\")"]}],"metadata":{"colab":{"provenance":[{"file_id":"1KVzYeLxt94SM4-D0GIFB1pzg09oShn1o","timestamp":1674333917802}],"toc_visible":true},"kernelspec":{"display_name":"base","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"},"vscode":{"interpreter":{"hash":"fb833273add3e7c60eb33c0608260b79a61e072ade6f02cc8d07b0a26eef8ab8"}},"accelerator":"GPU","gpuClass":"standard"},"nbformat":4,"nbformat_minor":0}